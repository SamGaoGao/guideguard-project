[![Concept Prototype](https://img.shields.io/badge/status-concept_prototype-orange?logo=idea)](https://github.com/samgaogao/guideguard)
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)

# GuideGuard (GG)  
> **Guide the generation. Guard the structure.**

A lightweight, non-invasive engine for LLM decoding â€” **zero training**, works out-of-the-box with **Llama, Qwen, GLM, Mistral, Gemma, Phi**, and any HuggingFace-compatible autoregressive model via `LogitsProcessor`.

Created by Gao Gao (é«˜é«˜) â€¢ MIT Licensed â€¢ Free for commercial use

---

## ğŸ’¡ Why GuideGuard?

Large language models often waste massive compute exploring tokens that **sound plausible in isolation but break sentence structure** â€” even when grammar makes the next wordâ€™s role obvious.

GuideGuard cuts that waste at the source with two real-time mechanisms:

### ğŸ§­ Stage 1: Guide â€” Focus Early, Save Compute  
When the grammatical role of the next word is already clear from context,  
thereâ€™s no need to sample from all 100k+ tokens.

- After **â€œTheâ€**, we expect a **noun** â€” not a pure verb like *â€œexplodeâ€*.  
  âœ… â€œThe explosion happened.â€  
  âŒ â€œThe explodeâ€¦â€  

- After Chinese passive marker **â€œè¢«â€**, we expect a **verb phrase** â€” not a bare noun like *â€œè‹¹æœâ€*.  
  âœ… â€œä»–è¢«æ‰¹è¯„äº†ã€‚â€  
  âŒ â€œä»–è¢«è‹¹æœã€‚â€

In these high-certainty moments, GuideGuard **shrinks the candidate pool** to only tokens matching the expected grammatical role.

â†’ **Fewer wasted samples, lower latency, less cost.**

### ğŸ›¡ï¸ Stage 2: Guard â€” Self-Correct as You Go  
After each token is generated, GG checks it against basic syntactic rules.

If a clear violation is detected â€” e.g., *â€œThe explodeâ€* or *â€œè¢« æ¡Œå­â€* â€”  
GG discards the token and asks the model to resample (up to 5 times).  
If no valid alternative exists, it safely falls back to the original output.

â†’ **Catches errors the moment they happen, not after hundreds of tokens.**

Because itâ€™s built on **universal grammatical roles** (NOUN, VERB, PASSIVE, etc.),  
GuideGuard is **language-agnostic** â€” extendable to English, Chinese, or any language.

> **Technical Note**: Language agnosticism requires providing:
> - A UPOS mapper (`token_id â†’ Universal POS tag`) for your tokenizer
> - Language-specific meta-rules (`(upos_{t-2}, upos_{t-1}) â†’ expected_upos_t`)

> âœ¨ **Value**: Lower compute cost + more robust output â€” with **zero training**.

---

## âš ï¸ This Is a Concept Prototype

While the core logic has been validated for correctness (state safety, batch processing, constraint activation), this release is **strictly a research prototype**:

- âŒ **Not production-ready**: You must implement key linguistic components yourself.
- âœ… **But logically sound**: No shared state, proper MCU detection, working fuses.
- ğŸ’¡ **Designed for collaboration**: A minimal, correct seed for community extension.

We prioritize **clarity over completeness** so you can understand, verify, and build upon the idea.

---

## ğŸ§© What You Must Implement

To use GuideGuard, provide three simple components:

1. **`upos_mapper`**: `token_id â†’ UPOS tag` (e.g., `"NOUN"`, `"VERB"`)  
   â†’ Map your tokenizerâ€™s IDs to universal POS tags.

2. **`upos_to_token_ids`**: `UPOS â†’ Set[token_id]`  
   â†’ Reverse lookup: which tokens belong to each grammatical class?

3. **`meta_rules`**: `{(upos_{t-2}, upos_{t-1}): expected_upos_t}`  
   â†’ Your language-specific syntactic rules (e.g., `("NOUN", "VERB") â†’ "NOUN"`).

These can be static dictionaries, rule-based systems, or even small classifiers â€” the interface is open.

---

## ğŸ§ª Demo

`demo.py` provides a **self-contained, runnable example** that validates the core logic:
- Shows how invalid tokens are blocked
- Demonstrates constraint activation after MCU detection
- Verifies batch safety and fuse behavior

Run it to see the â€œGuide & Guardâ€ effect in action.

---

## ğŸ”’ Safety & Design Principles

- **No shared state**: Each sequence in a batch is processed independently.
- **Basic fuses**: Prevent pathological over-constraint (e.g., too few allowed tokens).
- **HuggingFace native**: Implements `transformers.LogitsProcessor` â€” drop-in compatible.
- **Fail-safe**: Falls back to original logits if constraints are too restrictive.

---

## âš¡ Performance Note

This prototype prioritizes **conceptual clarity over speed**. For production use, consider:
- Caching UPOS mappings
- Vectorizing batch operations
- Precomputing allowed token sets per rule

---

## ğŸ“š How to Cite

If you use GuideGuard in academic or technical work, please cite it as software:

> **APA Format**: Gao, S. (2026). *GuideGuard: Delayed Syntactic Constraint via Minimal Complete Units* [Computer software]. https://github.com/samgaogao/guideguard

---

## ğŸŒ Vision

GuideGuard is more than code â€” it's a provocation:  
*What if LLMs could be both more creative and more efficient, simply by respecting the grammar they already know?*

---

## Â©ï¸ License

MIT License.  
Concept and implementation by Gao Gao (é«˜é«˜).  
Built for open research and collaboration â€” free for commercial and non-commercial use.
